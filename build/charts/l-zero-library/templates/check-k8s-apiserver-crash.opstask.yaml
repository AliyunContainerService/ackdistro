apiVersion: trident.apsara-stack.alibaba-inc.com/v1alpha1
kind: OpsTask
metadata:
  annotations:
    ark.alibaba-inc.com/name_cn: "apiserver\u5065\u5EB7\u68C0\u67E5"
  labels:
    bizType: checkTask
    check_appinstance: apiserver
    check_appset: k8s
    check_product: k8s
  name: check-k8s-apiserver-crash
  namespace: '{{.Values.k8s_namespace}}'
spec:
  broadcast: false
  description: "\u68C0\u67E5k8s Apiserver\u662F\u5426\u6709crash"
  period: 1900
  podSpec:
    containers:
    - command:
      - bash
      - -c
      - "#!/usr/bin/env bash\n\nsource /l0/utils/l0-utils.sh\n\ncheck_has_apiserver_crash()\
        \ {\n    if kubectl -n kube-system get pod -l component=kube-apiserver | grep\
        \ -w CrashLoopBackOff &>/dev/null;then\n        return 0\n    fi\n    return\
        \ 1\n}\n\ncheck_address_already_binded() {\n    rlt=\"\"\n    crash_pods=$(kubectl\
        \ -n kube-system get pod -l component=kube-apiserver | grep -w CrashLoopBackOff\
        \ | awk '{print $1}')\n    for item in ${crash_pods}; do\n        kubectl\
        \ -n kube-system logs ${item} -p --tail=10 | grep \"bind: address already\
        \ in use\" &>/dev/null\n        if [ $? -eq 0 ];then\n            rlt=${rlt}\"\
        ,\"${item}\n        fi\n    done\n    [[ \"$rlt\" == \"\" ]] && return 1\n\
        \    rlt=${rlt:1}\n    return 0\n}\n\nif ! check_has_apiserver_crash;then\n\
        \    Record \"K8S_APISERVER_CHECK\" \"k8s-apiserver\" \"pass\" \"Has no kube-apiserver\
        \ crash, this case is OK.\" \"\u68C0\u67E5K8s Apiserver\"\n    Return \"${TestResults}\"\
        \nfi\n\nif check_address_already_binded;then\n    Record \"K8S_APISERVER_CHECK\"\
        \ \"k8s-apiserver\" \"fail\" \"Apiserver pod[$rlt] crash, maybe because there\
        \ are duplicate apiserver process on the same node, please see doc:[https://work.aone.alibaba-inc.com/issue/30034272]\"\
        \ \"\u68C0\u67E5K8s Apiserver\"\n    Return \"${TestResults}\"\nfi\n\nRecord\
        \ \"K8S_APISERVER_CHECK\" \"k8s-apiserver\" \"fail\" \"There are some apiserver\
        \ pod[$rlt] crash in your cluster, but we can't find the reason, please ask\
        \ [\u4E13\u6709\u4E91\u654F\u6377PaaS\u5BB9\u5668\u6280\u672F\u652F\u6301\u7FA4\
        ] for help.\" \"\u68C0\u67E5K8s Apiserver\"\nReturn \"${TestResults}\""
      image: {{.Values.globalconfig.RegistryURL}}/{{.Values.images.opsbasealpine.image}}:{{.Values.images.opsbasealpine.tag}}
      imagePullPolicy: IfNotPresent
      name: main
    dnsPolicy: ClusterFirstWithHostNet
    hostNetwork: true
    tolerations:
    - effect: NoSchedule
      key: alibabacloud.com/system
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
  privilegeLevel: Cluster
  suspend: {{ .Values.globalconfig.SuspendPeriodHealthCheck }}
  timeout: 180
