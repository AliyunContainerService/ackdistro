apiVersion: apps/v1
kind: Deployment
metadata:
  name: hybridnet-manager
  namespace: kube-system
  labels:
    app: hybridnet
    component: manager
spec:
  replicas: {{ .Values.manager.replicas }}
  selector:
    matchLabels:
      app: hybridnet
      component: manager
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: hybridnet
        component: manager
    spec:
      tolerations:
        - operator: Exists
          effect: NoSchedule
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: hybridnet
                  component: manager
              topologyKey: kubernetes.io/hostname
      priorityClassName: system-cluster-critical
      serviceAccountName: hybridnet
      hostNetwork: true
      containers:
        - name: hybridnet-manager
          image: "{{ .Values.global.RegistryURL }}/{{ .Values.images.hybridnet.image }}:{{ .Values.images.hybridnet.tag }}"
          imagePullPolicy: {{ .Values.images.hybridnet.imagePullPolicy }}
          {{- if .Values.manager.resources }}
          resources:
            {{- toYaml .Values.manager.resources | trim | nindent 12 }}
          {{- end }}
          ports:
            {{- if .Values.manager.metricsPort }}
            - name: http-metrics
              containerPort: {{ .Values.manager.metricsPort }}
              protocol: TCP
            {{- end }}
          command:
            - /hybridnet/hybridnet-manager
            - --default-ip-retain={{ .Values.defaultIPRetain }}
            - --feature-gates=MultiCluster={{ .Values.multiCluster }},VMIPRetain={{ .Values.vmIPRetain }}
            {{- if .Values.manager.controllerConcurrency }}
            - --controller-concurrency={{ .Values.manager.controllerConcurrency }}
            {{- end }}
            {{- if .Values.manager.kubeClientQPS }}
            - --kube-client-qps={{ .Values.manager.kubeClientQPS }}
            {{- end }}
            {{- if .Values.manager.kubeClientBurst }}
            - --kube-client-burst={{ .Values.manager.kubeClientBurst }}
            {{- end }}
            {{- if .Values.manager.metricsPort }}
            - --metrics-port={{ .Values.manager.metricsPort }}
            {{- end }}
          env:
            - name: DEFAULT_NETWORK_TYPE
              value: {{ .Values.defaultNetworkType }}
            - name: DEFAULT_IP_FAMILY
              value: {{ .Values.defaultIPFamily }}
            - name: NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
      {{- if and .Values.manager .Values.manager.nodeSelector }}
      nodeSelector:
        {{- toYaml .Values.manager.nodeSelector | trim | nindent 8 }}
      {{- else }}
      # This will bring problems after k8s 1.24
      nodeSelector:
        node-role.kubernetes.io/master: ""
      {{- end }}

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hybridnet-webhook
  namespace: kube-system
  labels:
    app: hybridnet
    component: webhook
spec:
  replicas: {{ .Values.webhook.replicas }}
  selector:
    matchLabels:
      app: hybridnet
      component: webhook
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app: hybridnet
        component: webhook
        webhook.hybridnet.io/ignore: "true"
    spec:
      tolerations:
        - operator: Exists
          effect: NoSchedule
      {{- if and .Values.webhook .Values.webhook.nodeSelector }}
      nodeSelector:
        {{- toYaml .Values.webhook.nodeSelector | trim | nindent 8 }}
      {{- else }}
      # This will bring problems after k8s 1.24
      nodeSelector:
        node-role.kubernetes.io/master: ""
      {{- end }}
      priorityClassName: system-cluster-critical
      serviceAccountName: hybridnet
      hostNetwork: true
      containers:
        - name: hybridnet-webhook
          image: "{{ .Values.global.RegistryURL }}/{{ .Values.images.hybridnet.image }}:{{ .Values.images.hybridnet.tag }}"
          imagePullPolicy: {{ .Values.images.hybridnet.imagePullPolicy }}
          {{- if .Values.webhook.resources }}
          resources:
            {{- toYaml .Values.webhook.resources | trim | nindent 12 }}
          {{- end }}
          command:
            - /hybridnet/hybridnet-webhook
            - --default-ip-retain={{ .Values.defaultIPRetain }}
            - --feature-gates=MultiCluster={{ .Values.multiCluster }},VMIPRetain={{ .Values.vmIPRetain }}
          args:
            - --port=9898
          env:
            - name: DEFAULT_NETWORK_TYPE
              value: {{ .Values.defaultNetworkType }}
            - name: DEFAULT_IP_FAMILY
              value: {{ .Values.defaultIPFamily }}
          ports:
            - containerPort: 9898
              name: webhook-port

{{ if and .Values.typha .Values.daemon.enableFelixPolicy }}
---
# This manifest creates a Deployment of Typha to back the above service.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: calico-typha
  namespace: kube-system
  labels:
    app: hybridnet
    component: calico-typha
spec:
  # We recommend using Typha if you have more than 50 nodes.  Above 100 nodes it is essential
  # (when using the Kubernetes datastore).  Use one replica for every 100-200 nodes.  In
  # production, we recommend running at least 3 replicas to reduce the impact of rolling upgrade.
  replicas: {{ .Values.typha.replicas }}
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app: hybridnet
      component: calico-typha
  template:
    metadata:
      labels:
        app: hybridnet
        component: calico-typha
      annotations:
        # This, along with the CriticalAddonsOnly toleration below, marks the pod as a critical
        # add-on, ensuring it gets priority scheduling and that its resources are reserved
        # if it ever gets evicted.
        scheduler.alpha.kubernetes.io/critical-pod: ''
        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'
    spec:
      {{- if and .Values.typha .Values.typha.nodeSelector }}
      nodeSelector:
        {{- toYaml .Values.typha.nodeSelector | trim | nindent 8 }}
      {{- else }}
      nodeSelector:
        beta.kubernetes.io/os: linux
      {{- end }}
      hostNetwork: true
      tolerations:
        # Mark the pod as a critical add-on for rescheduling.
        - key: CriticalAddonsOnly
          operator: Exists
        - effect: NoSchedule
          operator: Exists
      # Since Calico can't network a pod until Typha is up, we need to run Typha itself
      # as a host-networked pod.
      serviceAccountName: hybridnet
      priorityClassName: system-cluster-critical
      # fsGroup allows using projected serviceaccount tokens as described here kubernetes/kubernetes#82573
      securityContext:
        fsGroup: 65534
        runAsUser: 1000
      containers:
        - name: typha
          image: "{{ .Values.global.RegistryURL }}/{{ .Values.images.hybridnet.image }}:{{ .Values.images.hybridnet.tag }}"
          command:
            - /hybridnet/policyinit.sh
          args:
            - calico-typha
          ports:
            - containerPort: {{ .Values.typha.serverPort }}
              name: calico-typha
              protocol: TCP
          env:
            # Enable "info" logging by default.  Can be set to "debug" to increase verbosity.
            - name: TYPHA_LOGSEVERITYSCREEN
              value: "info"
            # Disable logging to file and syslog since those don't make sense in Kubernetes.
            - name: TYPHA_LOGFILEPATH
              value: "none"
            - name: TYPHA_LOGSEVERITYSYS
              value: "none"
            # Monitor the Kubernetes API to find the number of running instances and rebalance
            # connections.
            - name: TYPHA_CONNECTIONREBALANCINGMODE
              value: "kubernetes"
            - name: TYPHA_DATASTORETYPE
              value: "kubernetes"
            - name: TYPHA_HEALTHENABLED
              value: "true"
            - name: TYPHA_MAXCONNECTIONSLOWERLIMIT
              value: "1"
            # Uncomment these lines to enable prometheus metrics.  Since Typha is host-networked,
            # this opens a port on the host, which may need to be secured.
            #- name: TYPHA_PROMETHEUSMETRICSENABLED
            #  value: "true"
            #- name: TYPHA_PROMETHEUSMETRICSPORT
            #  value: "9093"
            - name: ServerPort
              value: "{{ .Values.typha.serverPort }}"
          {{- if and .Values.typha .Values.typha.resources }}
          resources:
            {{- toYaml .Values.typha.resources | trim | nindent 12 }}
          {{- end }}
          livenessProbe:
            httpGet:
              path: /liveness
              port: 9098
              host: localhost
            periodSeconds: 30
            initialDelaySeconds: 30
          securityContext:
            runAsNonRoot: true
            allowPrivilegeEscalation: false
          readinessProbe:
            httpGet:
              path: /readiness
              port: 9098
              host: localhost
            periodSeconds: 10
{{ end }}
